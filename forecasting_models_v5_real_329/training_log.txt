Starting training of all models at Πεμ 03 Απρ 2025 08:29:35 πμ EEST
Models will be saved in /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5
All models will be trained with the new dataset format (default) and --use_stats flag enabled

=====================================================
Training linear model with config: basic
Starting at Πεμ 03 Απρ 2025 08:29:35 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type linear                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 
2025-04-03 08:29:35.797255: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:29:35.957783: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:29:36.583223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:29:36.583312: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:29:36.583322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:29:38.570000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:29:39.263568: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:29:39.263593: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:29:39.263729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4953 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Linear Regression model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 1)                 226       
                                                                 
=================================================================
Total params: 226
Trainable params: 226
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 8s - loss: 1.246813/13 [==============================] - 1s 14ms/step - loss: 0.5647 - val_loss: 0.3580 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.307213/13 [==============================] - 0s 5ms/step - loss: 0.2362 - val_loss: 0.7526 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.172113/13 [==============================] - 0s 5ms/step - loss: 0.2055 - val_loss: 0.3087 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.154513/13 [==============================] - 0s 5ms/step - loss: 0.1707 - val_loss: 0.2416 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.173613/13 [==============================] - 0s 5ms/step - loss: 0.1617 - val_loss: 0.2246 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.166813/13 [==============================] - 0s 5ms/step - loss: 0.1588 - val_loss: 0.2224 - lr: 0.0010
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.180613/13 [==============================] - 0s 5ms/step - loss: 0.1497 - val_loss: 0.2144 - lr: 0.0010
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.113013/13 [==============================] - 0s 5ms/step - loss: 0.1454 - val_loss: 0.2078 - lr: 0.0010
Epoch 9/20
 1/13 [=>............................] - ETA: 0s - loss: 0.148713/13 [==============================] - 0s 5ms/step - loss: 0.1413 - val_loss: 0.2030 - lr: 0.0010
Epoch 10/20
 1/13 [=>............................] - ETA: 0s - loss: 0.232213/13 [==============================] - 0s 5ms/step - loss: 0.1370 - val_loss: 0.1978 - lr: 0.0010
Epoch 11/20
 1/13 [=>............................] - ETA: 0s - loss: 0.104313/13 [==============================] - 0s 5ms/step - loss: 0.1328 - val_loss: 0.1959 - lr: 0.0010
Epoch 12/20
 1/13 [=>............................] - ETA: 0s - loss: 0.117913/13 [==============================] - 0s 5ms/step - loss: 0.1293 - val_loss: 0.1894 - lr: 0.0010
Epoch 13/20
 1/13 [=>............................] - ETA: 0s - loss: 0.140613/13 [==============================] - 0s 5ms/step - loss: 0.1248 - val_loss: 0.1864 - lr: 0.0010
Epoch 14/20
 1/13 [=>............................] - ETA: 0s - loss: 0.130613/13 [==============================] - 0s 5ms/step - loss: 0.1226 - val_loss: 0.1819 - lr: 0.0010
Epoch 15/20
 1/13 [=>............................] - ETA: 0s - loss: 0.114713/13 [==============================] - 0s 5ms/step - loss: 0.1194 - val_loss: 0.1770 - lr: 0.0010
Epoch 16/20
 1/13 [=>............................] - ETA: 0s - loss: 0.124113/13 [==============================] - 0s 5ms/step - loss: 0.1161 - val_loss: 0.1744 - lr: 0.0010
Epoch 17/20
 1/13 [=>............................] - ETA: 0s - loss: 0.121213/13 [==============================] - 0s 5ms/step - loss: 0.1131 - val_loss: 0.1704 - lr: 0.0010
Epoch 18/20
 1/13 [=>............................] - ETA: 0s - loss: 0.133913/13 [==============================] - 0s 5ms/step - loss: 0.1117 - val_loss: 0.1707 - lr: 0.0010
Epoch 19/20
 1/13 [=>............................] - ETA: 0s - loss: 0.060313/13 [==============================] - 0s 5ms/step - loss: 0.1074 - val_loss: 0.1648 - lr: 0.0010
Epoch 20/20
 1/13 [=>............................] - ETA: 0s - loss: 0.117613/13 [==============================] - 0s 5ms/step - loss: 0.1066 - val_loss: 0.1632 - lr: 0.0010
1/3 [=========>....................] - ETA: 0s - loss: 0.12213/3 [==============================] - 0s 2ms/step - loss: 0.1519
Test Loss: 0.15193717181682587
Saved model as forecasting_models_v5/model_linear_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LINEAR model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 63ms/step
Predicted QoE for the next time step: 79.89066174115182

Linear Model Feature Importance:
Traceback (most recent call last):
  File "timeseries_forecasting_models_v5.py", line 889, in <module>
    main()
  File "timeseries_forecasting_models_v5.py", line 886, in main
    print(f"{feature}: {weight[0]:.4f}")
IndexError: invalid index to scalar variable.
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/linear_basic.h5
Completed at Πεμ 03 Απρ 2025 08:29:44 πμ EEST
=====================================================

=====================================================
Training linear model with config: with_l1_reg
Starting at Πεμ 03 Απρ 2025 08:29:44 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type linear                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --l1_reg 0.01
2025-04-03 08:29:44.984826: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:29:45.129057: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:29:45.728536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:29:45.728614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:29:45.728624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:29:47.754006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:29:48.452388: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:29:48.452411: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:29:48.452574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4948 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Linear Regression model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 1)                 226       
                                                                 
=================================================================
Total params: 226
Trainable params: 226
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 9s - loss: 0.508513/13 [==============================] - 1s 15ms/step - loss: 0.4171 - val_loss: 0.7595 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.339413/13 [==============================] - 0s 5ms/step - loss: 0.3774 - val_loss: 0.4249 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.285313/13 [==============================] - 0s 5ms/step - loss: 0.3539 - val_loss: 0.4259 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.264013/13 [==============================] - 0s 5ms/step - loss: 0.3344 - val_loss: 0.3713 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.329713/13 [==============================] - 0s 5ms/step - loss: 0.3208 - val_loss: 0.3575 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.362613/13 [==============================] - 0s 5ms/step - loss: 0.3048 - val_loss: 0.4010 - lr: 0.0010
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.354713/13 [==============================] - 0s 5ms/step - loss: 0.2925 - val_loss: 0.3537 - lr: 0.0010
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.251713/13 [==============================] - 0s 5ms/step - loss: 0.2786 - val_loss: 0.3630 - lr: 0.0010
Epoch 9/20
 1/13 [=>............................] - ETA: 0s - loss: 0.237513/13 [==============================] - 0s 5ms/step - loss: 0.2701 - val_loss: 0.3253 - lr: 0.0010
Epoch 10/20
 1/13 [=>............................] - ETA: 0s - loss: 0.289313/13 [==============================] - 0s 5ms/step - loss: 0.2597 - val_loss: 0.3531 - lr: 0.0010
Epoch 11/20
 1/13 [=>............................] - ETA: 0s - loss: 0.197213/13 [==============================] - 0s 6ms/step - loss: 0.2505 - val_loss: 0.3418 - lr: 0.0010
Epoch 12/20
 1/13 [=>............................] - ETA: 0s - loss: 0.170413/13 [==============================] - 0s 6ms/step - loss: 0.2422 - val_loss: 0.3363 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.29193/3 [==============================] - 0s 2ms/step - loss: 0.2874
Test Loss: 0.28735262155532837
Saved model as forecasting_models_v5/model_linear_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LINEAR model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 64ms/step
Predicted QoE for the next time step: 78352.38349571289

Linear Model Feature Importance:
Traceback (most recent call last):
  File "timeseries_forecasting_models_v5.py", line 889, in <module>
    main()
  File "timeseries_forecasting_models_v5.py", line 886, in main
    print(f"{feature}: {weight[0]:.4f}")
IndexError: invalid index to scalar variable.
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/linear_with_l1_reg.h5
Completed at Πεμ 03 Απρ 2025 08:29:53 πμ EEST
=====================================================

=====================================================
Training linear model with config: with_l2_reg
Starting at Πεμ 03 Απρ 2025 08:29:53 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type linear                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --l2_reg 0.01
2025-04-03 08:29:53.776710: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:29:53.940018: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:29:54.561478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:29:54.561557: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:29:54.561578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:29:56.530987: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:29:57.202188: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:29:57.202213: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:29:57.202379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4952 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Linear Regression model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 1)                 226       
                                                                 
=================================================================
Total params: 226
Trainable params: 226
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 9s - loss: 0.123813/13 [==============================] - 1s 15ms/step - loss: 0.1661 - val_loss: 0.2721 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.101913/13 [==============================] - 0s 5ms/step - loss: 0.1501 - val_loss: 0.2715 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.124213/13 [==============================] - 0s 5ms/step - loss: 0.1378 - val_loss: 0.3398 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.114713/13 [==============================] - 0s 5ms/step - loss: 0.1335 - val_loss: 0.3070 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.151413/13 [==============================] - 0s 5ms/step - loss: 0.1275 - val_loss: 0.2535 - lr: 5.0000e-04
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.114913/13 [==============================] - 0s 5ms/step - loss: 0.1252 - val_loss: 0.3188 - lr: 5.0000e-04
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.147413/13 [==============================] - 0s 5ms/step - loss: 0.1221 - val_loss: 0.2763 - lr: 5.0000e-04
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.105713/13 [==============================] - 0s 6ms/step - loss: 0.1204 - val_loss: 0.2635 - lr: 2.5000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.17173/3 [==============================] - 0s 2ms/step - loss: 0.1656
Test Loss: 0.16564349830150604
Saved model as forecasting_models_v5/model_linear_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LINEAR model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 65ms/step
Predicted QoE for the next time step: 52662.42975536255

Linear Model Feature Importance:
Traceback (most recent call last):
  File "timeseries_forecasting_models_v5.py", line 889, in <module>
    main()
  File "timeseries_forecasting_models_v5.py", line 886, in main
    print(f"{feature}: {weight[0]:.4f}")
IndexError: invalid index to scalar variable.
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/linear_with_l2_reg.h5
Completed at Πεμ 03 Απρ 2025 08:30:01 πμ EEST
=====================================================

=====================================================
Training linear model with config: with_elastic_net
Starting at Πεμ 03 Απρ 2025 08:30:01 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type linear                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --l1_reg 0.005 --l2_reg 0.005
2025-04-03 08:30:02.231220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:02.383171: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:30:03.024723: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:03.024817: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:03.024829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:30:04.988127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:05.692389: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:30:05.692416: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:30:05.692575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4972 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Linear Regression model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 1)                 226       
                                                                 
=================================================================
Total params: 226
Trainable params: 226
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 9s - loss: 0.219813/13 [==============================] - 1s 17ms/step - loss: 0.2680 - val_loss: 0.4867 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.186313/13 [==============================] - 0s 5ms/step - loss: 0.2454 - val_loss: 0.3432 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.274913/13 [==============================] - 0s 5ms/step - loss: 0.2315 - val_loss: 0.2828 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.252213/13 [==============================] - 0s 5ms/step - loss: 0.2183 - val_loss: 0.2970 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.223513/13 [==============================] - 0s 5ms/step - loss: 0.2089 - val_loss: 0.2956 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.253513/13 [==============================] - 0s 6ms/step - loss: 0.2009 - val_loss: 0.3006 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.22193/3 [==============================] - 0s 2ms/step - loss: 0.2569
Test Loss: 0.2568615674972534
Saved model as forecasting_models_v5/model_linear_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LINEAR model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 64ms/step
Predicted QoE for the next time step: 6388.621652068939

Linear Model Feature Importance:
Traceback (most recent call last):
  File "timeseries_forecasting_models_v5.py", line 889, in <module>
    main()
  File "timeseries_forecasting_models_v5.py", line 886, in main
    print(f"{feature}: {weight[0]:.4f}")
IndexError: invalid index to scalar variable.
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/linear_with_elastic_net.h5
Completed at Πεμ 03 Απρ 2025 08:30:10 πμ EEST
=====================================================

=====================================================
Training dnn model with config: basic
Starting at Πεμ 03 Απρ 2025 08:30:10 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type dnn                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 
2025-04-03 08:30:10.700543: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:10.850208: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:30:11.481263: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:11.481353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:11.481362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:30:13.451028: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:14.128250: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:30:14.128286: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:30:14.128420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4972 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Simple DNN model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 64)                14464     
                                                                 
 dropout (Dropout)           (None, 64)                0         
                                                                 
 dense_1 (Dense)             (None, 32)                2080      
                                                                 
 dropout_1 (Dropout)         (None, 32)                0         
                                                                 
 dense_2 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 16,577
Trainable params: 16,577
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 11s - loss: 0.391913/13 [==============================] - 1s 18ms/step - loss: 0.1498 - val_loss: 0.2340 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.090613/13 [==============================] - 0s 6ms/step - loss: 0.0860 - val_loss: 0.0740 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.086913/13 [==============================] - 0s 6ms/step - loss: 0.0656 - val_loss: 0.1774 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.1044 9/13 [===================>..........] - ETA: 0s - loss: 0.067213/13 [==============================] - 0s 9ms/step - loss: 0.0662 - val_loss: 0.1177 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.052413/13 [==============================] - 0s 6ms/step - loss: 0.0604 - val_loss: 0.1343 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04123/3 [==============================] - 0s 2ms/step - loss: 0.0444
Test Loss: 0.04442371055483818
Saved model as forecasting_models_v5/model_dnn_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for DNN model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 72ms/step
Predicted QoE for the next time step: 26839.5449059729
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/dnn_basic.h5
Completed at Πεμ 03 Απρ 2025 08:30:18 πμ EEST
=====================================================

=====================================================
Training dnn model with config: deep
Starting at Πεμ 03 Απρ 2025 08:30:18 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type dnn                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --hidden_layers 128,64,32
2025-04-03 08:30:19.237625: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:19.400153: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:30:20.021668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:20.021764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:20.021774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:30:22.034978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:22.825728: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:30:22.825753: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:30:22.825897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4954 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Simple DNN model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 128)               28928     
                                                                 
 dropout (Dropout)           (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 64)                8256      
                                                                 
 dropout_1 (Dropout)         (None, 64)                0         
                                                                 
 dense_2 (Dense)             (None, 32)                2080      
                                                                 
 dropout_2 (Dropout)         (None, 32)                0         
                                                                 
 dense_3 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 39,297
Trainable params: 39,297
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 14s - loss: 0.653113/13 [==============================] - 1s 20ms/step - loss: 0.1728 - val_loss: 0.0523 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.113913/13 [==============================] - 0s 6ms/step - loss: 0.0816 - val_loss: 0.0865 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.080113/13 [==============================] - 0s 7ms/step - loss: 0.0635 - val_loss: 0.0504 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.072213/13 [==============================] - 0s 6ms/step - loss: 0.0547 - val_loss: 0.0571 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.042913/13 [==============================] - 0s 6ms/step - loss: 0.0545 - val_loss: 0.0444 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.019013/13 [==============================] - 0s 6ms/step - loss: 0.0527 - val_loss: 0.0455 - lr: 0.0010
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.034813/13 [==============================] - 0s 7ms/step - loss: 0.0554 - val_loss: 0.0491 - lr: 0.0010
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.063213/13 [==============================] - 0s 8ms/step - loss: 0.0450 - val_loss: 0.0457 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04703/3 [==============================] - 0s 2ms/step - loss: 0.0551
Test Loss: 0.0551462322473526
Saved model as forecasting_models_v5/model_dnn_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for DNN model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 84ms/step
Predicted QoE for the next time step: 12635.390004134215
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/dnn_deep.h5
Completed at Πεμ 03 Απρ 2025 08:30:27 πμ EEST
=====================================================

=====================================================
Training dnn model with config: with_elu
Starting at Πεμ 03 Απρ 2025 08:30:28 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type dnn                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --activation elu
2025-04-03 08:30:28.406255: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:28.565969: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:30:29.161902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:29.161991: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:29.162006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:30:31.199894: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:31.930153: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:30:31.930177: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:30:31.930315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4946 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Simple DNN model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 64)                14464     
                                                                 
 dropout (Dropout)           (None, 64)                0         
                                                                 
 dense_1 (Dense)             (None, 32)                2080      
                                                                 
 dropout_1 (Dropout)         (None, 32)                0         
                                                                 
 dense_2 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 16,577
Trainable params: 16,577
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 10s - loss: 0.192013/13 [==============================] - 1s 17ms/step - loss: 0.1447 - val_loss: 0.1029 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.125013/13 [==============================] - 0s 6ms/step - loss: 0.0959 - val_loss: 0.0708 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.134513/13 [==============================] - 0s 6ms/step - loss: 0.0775 - val_loss: 0.1003 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.054113/13 [==============================] - 0s 6ms/step - loss: 0.0742 - val_loss: 0.0773 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.074113/13 [==============================] - 0s 7ms/step - loss: 0.0739 - val_loss: 0.0555 - lr: 5.0000e-04
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.053013/13 [==============================] - 0s 6ms/step - loss: 0.0644 - val_loss: 0.0921 - lr: 5.0000e-04
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.067613/13 [==============================] - 0s 6ms/step - loss: 0.0594 - val_loss: 0.0808 - lr: 5.0000e-04
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.043513/13 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0665 - lr: 2.5000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.05783/3 [==============================] - 0s 2ms/step - loss: 0.0497
Test Loss: 0.04965755715966225
Saved model as forecasting_models_v5/model_dnn_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for DNN model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 88ms/step
Predicted QoE for the next time step: 390.0216400863075
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/dnn_with_elu.h5
Completed at Πεμ 03 Απρ 2025 08:30:36 πμ EEST
=====================================================

=====================================================
Training dnn model with config: with_high_dropout
Starting at Πεμ 03 Απρ 2025 08:30:36 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type dnn                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --dropout_rate 0.4
2025-04-03 08:30:37.289335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:37.452726: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:30:38.088295: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:38.088378: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:38.088399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:30:40.112077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:40.797734: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:30:40.797758: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:30:40.797903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4949 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Simple DNN model...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense (Dense)               (None, 64)                14464     
                                                                 
 dropout (Dropout)           (None, 64)                0         
                                                                 
 dense_1 (Dense)             (None, 32)                2080      
                                                                 
 dropout_1 (Dropout)         (None, 32)                0         
                                                                 
 dense_2 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 16,577
Trainable params: 16,577
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 11s - loss: 0.603713/13 [==============================] - 1s 20ms/step - loss: 0.2198 - val_loss: 0.3964 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.132813/13 [==============================] - 0s 6ms/step - loss: 0.1174 - val_loss: 0.0868 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.105213/13 [==============================] - 0s 6ms/step - loss: 0.1087 - val_loss: 0.0793 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.067013/13 [==============================] - 0s 6ms/step - loss: 0.0796 - val_loss: 0.0768 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.075913/13 [==============================] - 0s 6ms/step - loss: 0.0776 - val_loss: 0.0617 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.112213/13 [==============================] - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0714 - lr: 0.0010
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.066313/13 [==============================] - 0s 7ms/step - loss: 0.0773 - val_loss: 0.0639 - lr: 0.0010
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.072013/13 [==============================] - 0s 7ms/step - loss: 0.0716 - val_loss: 0.0659 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04223/3 [==============================] - 0s 2ms/step - loss: 0.0650
Test Loss: 0.06501211225986481
Saved model as forecasting_models_v5/model_dnn_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for DNN model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 78ms/step
Predicted QoE for the next time step: 14924.372014865721
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/dnn_with_high_dropout.h5
Completed at Πεμ 03 Απρ 2025 08:30:45 πμ EEST
=====================================================

=====================================================
Training lstm model with config: basic
Starting at Πεμ 03 Απρ 2025 08:30:45 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type lstm                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --attention_units 128
2025-04-03 08:30:46.255834: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:46.413586: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:30:47.042819: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:47.042901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:30:47.042911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:30:49.195908: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:30:49.925198: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:30:49.925221: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:30:49.925359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4928 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building LSTM model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 5, 50)             19200     
                                                                 
 lstm_1 (LSTM)               (None, 5, 50)             20200     
                                                                 
 self_attention (SelfAttenti  (None, 50)               6656      
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                1275      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 47,357
Trainable params: 47,357
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 53s - loss: 0.1682 3/13 [=====>........................] - ETA: 0s - loss: 0.1970  5/13 [==========>...................] - ETA: 0s - loss: 0.2012 7/13 [===============>..............] - ETA: 0s - loss: 0.1979 9/13 [===================>..........] - ETA: 0s - loss: 0.187710/13 [======================>.......] - ETA: 0s - loss: 0.179312/13 [==========================>...] - ETA: 0s - loss: 0.160013/13 [==============================] - 6s 99ms/step - loss: 0.1537 - val_loss: 0.1054 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0179 3/13 [=====>........................] - ETA: 0s - loss: 0.0344 5/13 [==========>...................] - ETA: 0s - loss: 0.0586 7/13 [===============>..............] - ETA: 0s - loss: 0.0660 9/13 [===================>..........] - ETA: 0s - loss: 0.059711/13 [========================>.....] - ETA: 0s - loss: 0.058212/13 [==========================>...] - ETA: 0s - loss: 0.055813/13 [==============================] - 1s 51ms/step - loss: 0.0545 - val_loss: 0.0506 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0540 2/13 [===>..........................] - ETA: 0s - loss: 0.0497 3/13 [=====>........................] - ETA: 0s - loss: 0.0495 4/13 [========>.....................] - ETA: 0s - loss: 0.0547 5/13 [==========>...................] - ETA: 0s - loss: 0.0554 7/13 [===============>..............] - ETA: 0s - loss: 0.0549 8/13 [=================>............] - ETA: 0s - loss: 0.0526 9/13 [===================>..........] - ETA: 0s - loss: 0.052010/13 [======================>.......] - ETA: 0s - loss: 0.054511/13 [========================>.....] - ETA: 0s - loss: 0.052912/13 [==========================>...] - ETA: 0s - loss: 0.052513/13 [==============================] - ETA: 0s - loss: 0.050913/13 [==============================] - 1s 58ms/step - loss: 0.0509 - val_loss: 0.0748 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0304 2/13 [===>..........................] - ETA: 0s - loss: 0.0263 3/13 [=====>........................] - ETA: 0s - loss: 0.0247 4/13 [========>.....................] - ETA: 0s - loss: 0.0340 5/13 [==========>...................] - ETA: 0s - loss: 0.0423 6/13 [============>.................] - ETA: 0s - loss: 0.0376 7/13 [===============>..............] - ETA: 0s - loss: 0.0477 8/13 [=================>............] - ETA: 0s - loss: 0.0444 9/13 [===================>..........] - ETA: 0s - loss: 0.045310/13 [======================>.......] - ETA: 0s - loss: 0.042512/13 [==========================>...] - ETA: 0s - loss: 0.043213/13 [==============================] - ETA: 0s - loss: 0.043813/13 [==============================] - 1s 61ms/step - loss: 0.0438 - val_loss: 0.0599 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0691 3/13 [=====>........................] - ETA: 0s - loss: 0.0444 5/13 [==========>...................] - ETA: 0s - loss: 0.0398 7/13 [===============>..............] - ETA: 0s - loss: 0.0382 8/13 [=================>............] - ETA: 0s - loss: 0.041410/13 [======================>.......] - ETA: 0s - loss: 0.041612/13 [==========================>...] - ETA: 0s - loss: 0.042513/13 [==============================] - ETA: 0s - loss: 0.042913/13 [==============================] - 1s 55ms/step - loss: 0.0429 - val_loss: 0.0581 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04343/3 [==============================] - 0s 10ms/step - loss: 0.0502
Test Loss: 0.05022227764129639
Saved model as forecasting_models_v5/model_lstm_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LSTM model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 498ms/step
Predicted QoE for the next time step: 60.39233320966601
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/lstm_basic.h5
Completed at Πεμ 03 Απρ 2025 08:31:02 πμ EEST
=====================================================

=====================================================
Training lstm model with config: deep
Starting at Πεμ 03 Απρ 2025 08:31:02 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type lstm                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --num_layers 3 --attention_units 128
2025-04-03 08:31:03.143184: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:03.330585: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:31:03.920947: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:03.921027: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:03.921036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:31:05.926093: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:06.638915: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:31:06.638941: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:31:06.639086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4971 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building LSTM model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 5, 50)             19200     
                                                                 
 lstm_1 (LSTM)               (None, 5, 50)             20200     
                                                                 
 lstm_2 (LSTM)               (None, 5, 50)             20200     
                                                                 
 self_attention (SelfAttenti  (None, 50)               6656      
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                1275      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 67,557
Trainable params: 67,557
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 1:16 - loss: 0.2311 2/13 [===>..........................] - ETA: 0s - loss: 0.2364   3/13 [=====>........................] - ETA: 0s - loss: 0.2402 4/13 [========>.....................] - ETA: 0s - loss: 0.2403 5/13 [==========>...................] - ETA: 0s - loss: 0.2356 6/13 [============>.................] - ETA: 0s - loss: 0.2329 7/13 [===============>..............] - ETA: 0s - loss: 0.2260 8/13 [=================>............] - ETA: 0s - loss: 0.2117 9/13 [===================>..........] - ETA: 0s - loss: 0.209710/13 [======================>.......] - ETA: 0s - loss: 0.201511/13 [========================>.....] - ETA: 0s - loss: 0.193912/13 [==========================>...] - ETA: 0s - loss: 0.185013/13 [==============================] - ETA: 0s - loss: 0.175213/13 [==============================] - 8s 141ms/step - loss: 0.1752 - val_loss: 0.1279 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0647 2/13 [===>..........................] - ETA: 0s - loss: 0.0442 3/13 [=====>........................] - ETA: 0s - loss: 0.0612 4/13 [========>.....................] - ETA: 0s - loss: 0.0761 5/13 [==========>...................] - ETA: 0s - loss: 0.0743 6/13 [============>.................] - ETA: 0s - loss: 0.0743 7/13 [===============>..............] - ETA: 0s - loss: 0.0699 8/13 [=================>............] - ETA: 0s - loss: 0.0714 9/13 [===================>..........] - ETA: 0s - loss: 0.071010/13 [======================>.......] - ETA: 0s - loss: 0.069211/13 [========================>.....] - ETA: 0s - loss: 0.067312/13 [==========================>...] - ETA: 0s - loss: 0.066513/13 [==============================] - ETA: 0s - loss: 0.066013/13 [==============================] - 1s 80ms/step - loss: 0.0660 - val_loss: 0.0497 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0638 2/13 [===>..........................] - ETA: 0s - loss: 0.0670 3/13 [=====>........................] - ETA: 0s - loss: 0.0664 4/13 [========>.....................] - ETA: 0s - loss: 0.0625 5/13 [==========>...................] - ETA: 0s - loss: 0.0627 6/13 [============>.................] - ETA: 0s - loss: 0.0652 7/13 [===============>..............] - ETA: 0s - loss: 0.0630 8/13 [=================>............] - ETA: 0s - loss: 0.0631 9/13 [===================>..........] - ETA: 0s - loss: 0.062610/13 [======================>.......] - ETA: 0s - loss: 0.061111/13 [========================>.....] - ETA: 0s - loss: 0.061312/13 [==========================>...] - ETA: 0s - loss: 0.060013/13 [==============================] - ETA: 0s - loss: 0.057713/13 [==============================] - 1s 80ms/step - loss: 0.0577 - val_loss: 0.0831 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0544 2/13 [===>..........................] - ETA: 0s - loss: 0.0438 3/13 [=====>........................] - ETA: 0s - loss: 0.0358 4/13 [========>.....................] - ETA: 0s - loss: 0.0407 5/13 [==========>...................] - ETA: 0s - loss: 0.0419 6/13 [============>.................] - ETA: 0s - loss: 0.0435 7/13 [===============>..............] - ETA: 0s - loss: 0.0452 8/13 [=================>............] - ETA: 0s - loss: 0.0434 9/13 [===================>..........] - ETA: 0s - loss: 0.046310/13 [======================>.......] - ETA: 0s - loss: 0.046611/13 [========================>.....] - ETA: 0s - loss: 0.045512/13 [==========================>...] - ETA: 0s - loss: 0.047613/13 [==============================] - ETA: 0s - loss: 0.046213/13 [==============================] - 1s 81ms/step - loss: 0.0462 - val_loss: 0.0567 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 1s - loss: 0.0389 2/13 [===>..........................] - ETA: 0s - loss: 0.0393 3/13 [=====>........................] - ETA: 0s - loss: 0.0398 4/13 [========>.....................] - ETA: 0s - loss: 0.0389 5/13 [==========>...................] - ETA: 0s - loss: 0.0467 6/13 [============>.................] - ETA: 0s - loss: 0.0458 7/13 [===============>..............] - ETA: 0s - loss: 0.0460 8/13 [=================>............] - ETA: 0s - loss: 0.0450 9/13 [===================>..........] - ETA: 0s - loss: 0.046610/13 [======================>.......] - ETA: 0s - loss: 0.046411/13 [========================>.....] - ETA: 0s - loss: 0.045812/13 [==========================>...] - ETA: 0s - loss: 0.045213/13 [==============================] - ETA: 0s - loss: 0.044113/13 [==============================] - 1s 82ms/step - loss: 0.0441 - val_loss: 0.0557 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04513/3 [==============================] - 0s 14ms/step - loss: 0.0525
Test Loss: 0.05252058804035187
Saved model as forecasting_models_v5/model_lstm_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LSTM model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 676ms/step
Predicted QoE for the next time step: 59.906722191836835
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/lstm_deep.h5
Completed at Πεμ 03 Απρ 2025 08:31:23 πμ EEST
=====================================================

=====================================================
Training lstm model with config: wide
Starting at Πεμ 03 Απρ 2025 08:31:23 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type lstm                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --hidden_units 100 --attention_units 128
2025-04-03 08:31:23.787320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:23.995486: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:31:24.625877: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:24.625959: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:24.625975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:31:26.644901: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:27.368731: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:31:27.368754: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:31:27.368927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4971 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building LSTM model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 5, 100)            58400     
                                                                 
 lstm_1 (LSTM)               (None, 5, 100)            80400     
                                                                 
 self_attention (SelfAttenti  (None, 100)              13056     
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                2525      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 154,407
Trainable params: 154,407
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 52s - loss: 0.2244 3/13 [=====>........................] - ETA: 0s - loss: 0.2067  5/13 [==========>...................] - ETA: 0s - loss: 0.1788 7/13 [===============>..............] - ETA: 0s - loss: 0.1549 8/13 [=================>............] - ETA: 0s - loss: 0.139110/13 [======================>.......] - ETA: 0s - loss: 0.121112/13 [==========================>...] - ETA: 0s - loss: 0.116613/13 [==============================] - ETA: 0s - loss: 0.111513/13 [==============================] - 6s 100ms/step - loss: 0.1115 - val_loss: 0.1111 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0204 3/13 [=====>........................] - ETA: 0s - loss: 0.0473 5/13 [==========>...................] - ETA: 0s - loss: 0.0495 7/13 [===============>..............] - ETA: 0s - loss: 0.0471 9/13 [===================>..........] - ETA: 0s - loss: 0.050811/13 [========================>.....] - ETA: 0s - loss: 0.049513/13 [==============================] - ETA: 0s - loss: 0.046513/13 [==============================] - 1s 50ms/step - loss: 0.0465 - val_loss: 0.0538 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0368 3/13 [=====>........................] - ETA: 0s - loss: 0.0351 5/13 [==========>...................] - ETA: 0s - loss: 0.0423 7/13 [===============>..............] - ETA: 0s - loss: 0.0432 8/13 [=================>............] - ETA: 0s - loss: 0.0454 9/13 [===================>..........] - ETA: 0s - loss: 0.042910/13 [======================>.......] - ETA: 0s - loss: 0.042011/13 [========================>.....] - ETA: 0s - loss: 0.043812/13 [==========================>...] - ETA: 0s - loss: 0.043413/13 [==============================] - ETA: 0s - loss: 0.043313/13 [==============================] - 1s 57ms/step - loss: 0.0433 - val_loss: 0.0538 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0227 2/13 [===>..........................] - ETA: 0s - loss: 0.0316 3/13 [=====>........................] - ETA: 0s - loss: 0.0278 4/13 [========>.....................] - ETA: 0s - loss: 0.0267 5/13 [==========>...................] - ETA: 0s - loss: 0.0304 6/13 [============>.................] - ETA: 0s - loss: 0.0350 7/13 [===============>..............] - ETA: 0s - loss: 0.0348 8/13 [=================>............] - ETA: 0s - loss: 0.035510/13 [======================>.......] - ETA: 0s - loss: 0.041111/13 [========================>.....] - ETA: 0s - loss: 0.041012/13 [==========================>...] - ETA: 0s - loss: 0.041013/13 [==============================] - 1s 57ms/step - loss: 0.0410 - val_loss: 0.0559 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0421 2/13 [===>..........................] - ETA: 0s - loss: 0.0340 4/13 [========>.....................] - ETA: 0s - loss: 0.0419 6/13 [============>.................] - ETA: 0s - loss: 0.0413 8/13 [=================>............] - ETA: 0s - loss: 0.041310/13 [======================>.......] - ETA: 0s - loss: 0.041011/13 [========================>.....] - ETA: 0s - loss: 0.041313/13 [==============================] - ETA: 0s - loss: 0.041213/13 [==============================] - 1s 56ms/step - loss: 0.0412 - val_loss: 0.0555 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04093/3 [==============================] - 0s 11ms/step - loss: 0.0468
Test Loss: 0.0468025840818882
Saved model as forecasting_models_v5/model_lstm_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LSTM model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 539ms/step
Predicted QoE for the next time step: 62.02067093152046
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/lstm_wide.h5
Completed at Πεμ 03 Απρ 2025 08:31:40 πμ EEST
=====================================================

=====================================================
Training lstm model with config: bidirectional
Starting at Πεμ 03 Απρ 2025 08:31:40 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type lstm                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --bidirectional --attention_units 128
2025-04-03 08:31:40.457001: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:40.605033: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:31:41.195486: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:41.195571: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:41.195581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:31:43.162893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:43.839504: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:31:43.839539: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:31:43.839663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4971 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building LSTM model with self-attention...
Traceback (most recent call last):
  File "timeseries_forecasting_models_v5.py", line 889, in <module>
    main()
  File "timeseries_forecasting_models_v5.py", line 720, in main
    model.summary()
  File "/home/dimitris/.local/lib/python3.8/site-packages/keras/engine/training.py", line 3214, in summary
    raise ValueError(
ValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.
Warning: Original model file not found at /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/model_lstm_with_attention.h5
Directory contents:
total 4772
drwxrwxr-x  2 dimitris dimitris    4096 Απρ   3 08:31 .
drwxrwxr-x 11 dimitris dimitris    4096 Απρ   3 08:28 ..
-rw-rw-r--  1 dimitris dimitris  233792 Απρ   3 08:30 dnn_basic.h5
-rw-rw-r--  1 dimitris dimitris  519208 Απρ   3 08:30 dnn_deep.h5
-rw-rw-r--  1 dimitris dimitris  233792 Απρ   3 08:30 dnn_with_elu.h5
-rw-rw-r--  1 dimitris dimitris  233792 Απρ   3 08:30 dnn_with_high_dropout.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:29 linear_basic.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:30 linear_with_elastic_net.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:29 linear_with_l1_reg.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:29 linear_with_l2_reg.h5
-rw-rw-r--  1 dimitris dimitris  628760 Απρ   3 08:30 lstm_basic.h5
-rw-rw-r--  1 dimitris dimitris  881952 Απρ   3 08:31 lstm_deep.h5
-rw-rw-r--  1 dimitris dimitris 1914264 Απρ   3 08:31 lstm_wide.h5
-rw-rw-r--  1 dimitris dimitris    3463 Απρ   3 08:31 scaler.save
-rw-rw-r--  1 dimitris dimitris  103973 Απρ   3 08:31 training_log.txt
Completed at Πεμ 03 Απρ 2025 08:31:46 πμ EEST
=====================================================

=====================================================
Training lstm model with config: with_stats
Starting at Πεμ 03 Απρ 2025 08:31:46 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type lstm                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --attention_units 128
2025-04-03 08:31:46.933275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:47.094394: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:31:47.709010: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:47.709089: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:31:47.709099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:31:49.668600: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:31:50.361538: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:31:50.361574: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:31:50.361735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4966 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building LSTM model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 5, 50)             19200     
                                                                 
 lstm_1 (LSTM)               (None, 5, 50)             20200     
                                                                 
 self_attention (SelfAttenti  (None, 50)               6656      
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                1275      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 47,357
Trainable params: 47,357
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 52s - loss: 0.2743 3/13 [=====>........................] - ETA: 0s - loss: 0.2488  5/13 [==========>...................] - ETA: 0s - loss: 0.2336 7/13 [===============>..............] - ETA: 0s - loss: 0.2247 9/13 [===================>..........] - ETA: 0s - loss: 0.221911/13 [========================>.....] - ETA: 0s - loss: 0.205812/13 [==========================>...] - ETA: 0s - loss: 0.198813/13 [==============================] - 6s 95ms/step - loss: 0.1903 - val_loss: 0.0575 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0651 3/13 [=====>........................] - ETA: 0s - loss: 0.0503 4/13 [========>.....................] - ETA: 0s - loss: 0.0584 5/13 [==========>...................] - ETA: 0s - loss: 0.0561 6/13 [============>.................] - ETA: 0s - loss: 0.0605 7/13 [===============>..............] - ETA: 0s - loss: 0.0623 9/13 [===================>..........] - ETA: 0s - loss: 0.061611/13 [========================>.....] - ETA: 0s - loss: 0.057613/13 [==============================] - ETA: 0s - loss: 0.059213/13 [==============================] - 1s 57ms/step - loss: 0.0592 - val_loss: 0.0693 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0567 3/13 [=====>........................] - ETA: 0s - loss: 0.0538 4/13 [========>.....................] - ETA: 0s - loss: 0.0507 6/13 [============>.................] - ETA: 0s - loss: 0.0509 8/13 [=================>............] - ETA: 0s - loss: 0.052010/13 [======================>.......] - ETA: 0s - loss: 0.050911/13 [========================>.....] - ETA: 0s - loss: 0.051713/13 [==============================] - ETA: 0s - loss: 0.050413/13 [==============================] - 1s 55ms/step - loss: 0.0504 - val_loss: 0.0707 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0411 2/13 [===>..........................] - ETA: 0s - loss: 0.0303 4/13 [========>.....................] - ETA: 0s - loss: 0.0398 5/13 [==========>...................] - ETA: 0s - loss: 0.0400 6/13 [============>.................] - ETA: 0s - loss: 0.0381 8/13 [=================>............] - ETA: 0s - loss: 0.039310/13 [======================>.......] - ETA: 0s - loss: 0.042912/13 [==========================>...] - ETA: 0s - loss: 0.044613/13 [==============================] - 1s 57ms/step - loss: 0.0441 - val_loss: 0.0635 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.03983/3 [==============================] - 0s 9ms/step - loss: 0.0452
Test Loss: 0.04516632854938507
Saved model as forecasting_models_v5/model_lstm_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for LSTM model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 546ms/step
Predicted QoE for the next time step: 68.19797719800711
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/lstm_with_stats.h5
Completed at Πεμ 03 Απρ 2025 08:32:02 πμ EEST
=====================================================

=====================================================
Training gru model with config: basic
Starting at Πεμ 03 Απρ 2025 08:32:02 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type gru                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --attention_units 128
2025-04-03 08:32:02.846313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:32:03.035475: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:32:03.650827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:32:03.650908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:32:03.650929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:32:05.645222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:32:06.331282: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:32:06.331306: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:32:06.331441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4969 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building GRU model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 gru (GRU)                   (None, 5, 50)             14550     
                                                                 
 gru_1 (GRU)                 (None, 5, 50)             15300     
                                                                 
 self_attention (SelfAttenti  (None, 50)               6656      
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                1275      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 37,807
Trainable params: 37,807
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 55s - loss: 0.2486 2/13 [===>..........................] - ETA: 0s - loss: 0.2288  4/13 [========>.....................] - ETA: 0s - loss: 0.2164 6/13 [============>.................] - ETA: 0s - loss: 0.1899 8/13 [=================>............] - ETA: 0s - loss: 0.162210/13 [======================>.......] - ETA: 0s - loss: 0.139511/13 [========================>.....] - ETA: 0s - loss: 0.131112/13 [==========================>...] - ETA: 0s - loss: 0.124513/13 [==============================] - 6s 109ms/step - loss: 0.1203 - val_loss: 0.1161 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0562 3/13 [=====>........................] - ETA: 0s - loss: 0.0572 5/13 [==========>...................] - ETA: 0s - loss: 0.0579 6/13 [============>.................] - ETA: 0s - loss: 0.0597 7/13 [===============>..............] - ETA: 0s - loss: 0.0562 8/13 [=================>............] - ETA: 0s - loss: 0.0556 9/13 [===================>..........] - ETA: 0s - loss: 0.054211/13 [========================>.....] - ETA: 0s - loss: 0.051013/13 [==============================] - ETA: 0s - loss: 0.051813/13 [==============================] - 1s 56ms/step - loss: 0.0518 - val_loss: 0.0491 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0514 3/13 [=====>........................] - ETA: 0s - loss: 0.0365 5/13 [==========>...................] - ETA: 0s - loss: 0.0373 6/13 [============>.................] - ETA: 0s - loss: 0.0363 8/13 [=================>............] - ETA: 0s - loss: 0.042510/13 [======================>.......] - ETA: 0s - loss: 0.044512/13 [==========================>...] - ETA: 0s - loss: 0.046113/13 [==============================] - 1s 54ms/step - loss: 0.0450 - val_loss: 0.0720 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0684 2/13 [===>..........................] - ETA: 0s - loss: 0.0559 3/13 [=====>........................] - ETA: 0s - loss: 0.0510 5/13 [==========>...................] - ETA: 0s - loss: 0.0517 6/13 [============>.................] - ETA: 0s - loss: 0.0485 7/13 [===============>..............] - ETA: 0s - loss: 0.0471 9/13 [===================>..........] - ETA: 0s - loss: 0.044511/13 [========================>.....] - ETA: 0s - loss: 0.044413/13 [==============================] - ETA: 0s - loss: 0.043613/13 [==============================] - 1s 54ms/step - loss: 0.0436 - val_loss: 0.0578 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0725 3/13 [=====>........................] - ETA: 0s - loss: 0.0571 5/13 [==========>...................] - ETA: 0s - loss: 0.0489 7/13 [===============>..............] - ETA: 0s - loss: 0.0443 8/13 [=================>............] - ETA: 0s - loss: 0.041810/13 [======================>.......] - ETA: 0s - loss: 0.046112/13 [==========================>...] - ETA: 0s - loss: 0.042813/13 [==============================] - ETA: 0s - loss: 0.041413/13 [==============================] - 1s 57ms/step - loss: 0.0414 - val_loss: 0.0603 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04453/3 [==============================] - 0s 9ms/step - loss: 0.0520
Test Loss: 0.0519697368144989
Saved model as forecasting_models_v5/model_gru_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for GRU model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 723ms/step
Predicted QoE for the next time step: 52.86087969956637
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/gru_basic.h5
Completed at Πεμ 03 Απρ 2025 08:32:19 πμ EEST
=====================================================

=====================================================
Training gru model with config: deep
Starting at Πεμ 03 Απρ 2025 08:32:19 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type gru                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --num_layers 3 --attention_units 128
2025-04-03 08:32:20.068283: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:32:20.235048: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:32:20.914247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:32:20.914341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:32:20.914352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:32:23.109295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:32:23.843507: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:32:23.843534: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:32:23.843677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4976 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building GRU model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 gru (GRU)                   (None, 5, 50)             14550     
                                                                 
 gru_1 (GRU)                 (None, 5, 50)             15300     
                                                                 
 gru_2 (GRU)                 (None, 5, 50)             15300     
                                                                 
 self_attention (SelfAttenti  (None, 50)               6656      
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                1275      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 53,107
Trainable params: 53,107
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 1:16 - loss: 0.2033 2/13 [===>..........................] - ETA: 0s - loss: 0.2040   3/13 [=====>........................] - ETA: 0s - loss: 0.1827 4/13 [========>.....................] - ETA: 0s - loss: 0.1661 5/13 [==========>...................] - ETA: 0s - loss: 0.1428 6/13 [============>.................] - ETA: 0s - loss: 0.1258 7/13 [===============>..............] - ETA: 0s - loss: 0.1185 8/13 [=================>............] - ETA: 0s - loss: 0.1063 9/13 [===================>..........] - ETA: 0s - loss: 0.105110/13 [======================>.......] - ETA: 0s - loss: 0.099411/13 [========================>.....] - ETA: 0s - loss: 0.098112/13 [==========================>...] - ETA: 0s - loss: 0.100813/13 [==============================] - ETA: 0s - loss: 0.096713/13 [==============================] - 8s 142ms/step - loss: 0.0967 - val_loss: 0.1036 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0458 2/13 [===>..........................] - ETA: 0s - loss: 0.0387 3/13 [=====>........................] - ETA: 0s - loss: 0.0395 4/13 [========>.....................] - ETA: 0s - loss: 0.0401 5/13 [==========>...................] - ETA: 0s - loss: 0.0444 6/13 [============>.................] - ETA: 0s - loss: 0.0473 7/13 [===============>..............] - ETA: 0s - loss: 0.0474 8/13 [=================>............] - ETA: 0s - loss: 0.0500 9/13 [===================>..........] - ETA: 0s - loss: 0.051010/13 [======================>.......] - ETA: 0s - loss: 0.048711/13 [========================>.....] - ETA: 0s - loss: 0.049012/13 [==========================>...] - ETA: 0s - loss: 0.048313/13 [==============================] - ETA: 0s - loss: 0.048413/13 [==============================] - 1s 73ms/step - loss: 0.0484 - val_loss: 0.0767 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 1s - loss: 0.0413 2/13 [===>..........................] - ETA: 0s - loss: 0.0323 3/13 [=====>........................] - ETA: 0s - loss: 0.0434 4/13 [========>.....................] - ETA: 0s - loss: 0.0482 5/13 [==========>...................] - ETA: 0s - loss: 0.0494 6/13 [============>.................] - ETA: 0s - loss: 0.0447 7/13 [===============>..............] - ETA: 0s - loss: 0.0479 8/13 [=================>............] - ETA: 0s - loss: 0.0472 9/13 [===================>..........] - ETA: 0s - loss: 0.045710/13 [======================>.......] - ETA: 0s - loss: 0.045411/13 [========================>.....] - ETA: 0s - loss: 0.043612/13 [==========================>...] - ETA: 0s - loss: 0.042713/13 [==============================] - ETA: 0s - loss: 0.042313/13 [==============================] - 1s 81ms/step - loss: 0.0423 - val_loss: 0.0679 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0452 2/13 [===>..........................] - ETA: 0s - loss: 0.0592 3/13 [=====>........................] - ETA: 0s - loss: 0.0449 4/13 [========>.....................] - ETA: 0s - loss: 0.0428 5/13 [==========>...................] - ETA: 0s - loss: 0.0381 6/13 [============>.................] - ETA: 0s - loss: 0.0425 7/13 [===============>..............] - ETA: 0s - loss: 0.0401 8/13 [=================>............] - ETA: 0s - loss: 0.0397 9/13 [===================>..........] - ETA: 0s - loss: 0.040910/13 [======================>.......] - ETA: 0s - loss: 0.039611/13 [========================>.....] - ETA: 0s - loss: 0.039312/13 [==========================>...] - ETA: 0s - loss: 0.041913/13 [==============================] - ETA: 0s - loss: 0.042113/13 [==============================] - 1s 78ms/step - loss: 0.0421 - val_loss: 0.0721 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0289 2/13 [===>..........................] - ETA: 0s - loss: 0.0313 3/13 [=====>........................] - ETA: 0s - loss: 0.0350 4/13 [========>.....................] - ETA: 0s - loss: 0.0388 5/13 [==========>...................] - ETA: 0s - loss: 0.0375 6/13 [============>.................] - ETA: 0s - loss: 0.0413 7/13 [===============>..............] - ETA: 0s - loss: 0.0448 8/13 [=================>............] - ETA: 0s - loss: 0.0458 9/13 [===================>..........] - ETA: 0s - loss: 0.045310/13 [======================>.......] - ETA: 0s - loss: 0.043911/13 [========================>.....] - ETA: 0s - loss: 0.043612/13 [==========================>...] - ETA: 0s - loss: 0.041913/13 [==============================] - ETA: 0s - loss: 0.043413/13 [==============================] - 1s 79ms/step - loss: 0.0434 - val_loss: 0.0712 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0361 2/13 [===>..........................] - ETA: 0s - loss: 0.0453 3/13 [=====>........................] - ETA: 0s - loss: 0.0552 4/13 [========>.....................] - ETA: 0s - loss: 0.0531 5/13 [==========>...................] - ETA: 0s - loss: 0.0493 6/13 [============>.................] - ETA: 0s - loss: 0.0510 7/13 [===============>..............] - ETA: 0s - loss: 0.0484 8/13 [=================>............] - ETA: 0s - loss: 0.0462 9/13 [===================>..........] - ETA: 0s - loss: 0.043210/13 [======================>.......] - ETA: 0s - loss: 0.043811/13 [========================>.....] - ETA: 0s - loss: 0.042412/13 [==========================>...] - ETA: 0s - loss: 0.042013/13 [==============================] - ETA: 0s - loss: 0.043913/13 [==============================] - 1s 77ms/step - loss: 0.0439 - val_loss: 0.0674 - lr: 5.0000e-04
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0463 2/13 [===>..........................] - ETA: 0s - loss: 0.0452 3/13 [=====>........................] - ETA: 0s - loss: 0.0416 4/13 [========>.....................] - ETA: 0s - loss: 0.0389 5/13 [==========>...................] - ETA: 0s - loss: 0.0445 6/13 [============>.................] - ETA: 0s - loss: 0.0437 7/13 [===============>..............] - ETA: 0s - loss: 0.0418 8/13 [=================>............] - ETA: 0s - loss: 0.0402 9/13 [===================>..........] - ETA: 0s - loss: 0.042710/13 [======================>.......] - ETA: 0s - loss: 0.044211/13 [========================>.....] - ETA: 0s - loss: 0.042712/13 [==========================>...] - ETA: 0s - loss: 0.042313/13 [==============================] - ETA: 0s - loss: 0.042113/13 [==============================] - 1s 78ms/step - loss: 0.0421 - val_loss: 0.0589 - lr: 5.0000e-04
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0327 2/13 [===>..........................] - ETA: 0s - loss: 0.0320 3/13 [=====>........................] - ETA: 0s - loss: 0.0310 4/13 [========>.....................] - ETA: 0s - loss: 0.0293 5/13 [==========>...................] - ETA: 0s - loss: 0.0388 6/13 [============>.................] - ETA: 0s - loss: 0.0353 7/13 [===============>..............] - ETA: 0s - loss: 0.0368 8/13 [=================>............] - ETA: 0s - loss: 0.0359 9/13 [===================>..........] - ETA: 0s - loss: 0.038810/13 [======================>.......] - ETA: 0s - loss: 0.038111/13 [========================>.....] - ETA: 0s - loss: 0.041512/13 [==========================>...] - ETA: 0s - loss: 0.041013/13 [==============================] - ETA: 0s - loss: 0.040813/13 [==============================] - 1s 80ms/step - loss: 0.0408 - val_loss: 0.0660 - lr: 5.0000e-04
Epoch 9/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0564 2/13 [===>..........................] - ETA: 0s - loss: 0.0428 3/13 [=====>........................] - ETA: 0s - loss: 0.0522 4/13 [========>.....................] - ETA: 0s - loss: 0.0507 5/13 [==========>...................] - ETA: 0s - loss: 0.0510 6/13 [============>.................] - ETA: 0s - loss: 0.0506 7/13 [===============>..............] - ETA: 0s - loss: 0.0501 8/13 [=================>............] - ETA: 0s - loss: 0.0509 9/13 [===================>..........] - ETA: 0s - loss: 0.048710/13 [======================>.......] - ETA: 0s - loss: 0.046811/13 [========================>.....] - ETA: 0s - loss: 0.046112/13 [==========================>...] - ETA: 0s - loss: 0.044513/13 [==============================] - ETA: 0s - loss: 0.044413/13 [==============================] - 1s 77ms/step - loss: 0.0444 - val_loss: 0.0666 - lr: 5.0000e-04
Epoch 10/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0379 2/13 [===>..........................] - ETA: 0s - loss: 0.0306 3/13 [=====>........................] - ETA: 0s - loss: 0.0389 4/13 [========>.....................] - ETA: 0s - loss: 0.0435 5/13 [==========>...................] - ETA: 0s - loss: 0.0423 6/13 [============>.................] - ETA: 0s - loss: 0.0419 7/13 [===============>..............] - ETA: 0s - loss: 0.0385 8/13 [=================>............] - ETA: 0s - loss: 0.0400 9/13 [===================>..........] - ETA: 0s - loss: 0.039310/13 [======================>.......] - ETA: 0s - loss: 0.038311/13 [========================>.....] - ETA: 0s - loss: 0.038912/13 [==========================>...] - ETA: 0s - loss: 0.042013/13 [==============================] - ETA: 0s - loss: 0.041913/13 [==============================] - 1s 78ms/step - loss: 0.0419 - val_loss: 0.0696 - lr: 2.5000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04183/3 [==============================] - 0s 11ms/step - loss: 0.0446
Test Loss: 0.044552627950906754
Saved model as forecasting_models_v5/model_gru_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for GRU model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 914ms/step
Predicted QoE for the next time step: 78.29876919880748
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/gru_deep.h5
Completed at Πεμ 03 Απρ 2025 08:32:45 πμ EEST
=====================================================

=====================================================
Training gru model with config: wide
Starting at Πεμ 03 Απρ 2025 08:32:45 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type gru                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --hidden_units 100 --attention_units 128
2025-04-03 08:32:46.195864: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:32:46.404937: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:32:47.121638: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:32:47.121789: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:32:47.121810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:32:49.270637: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:32:49.984677: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:32:49.984713: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:32:49.984851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4984 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building GRU model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 gru (GRU)                   (None, 5, 100)            44100     
                                                                 
 gru_1 (GRU)                 (None, 5, 100)            60600     
                                                                 
 self_attention (SelfAttenti  (None, 100)              13056     
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                2525      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 120,307
Trainable params: 120,307
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 54s - loss: 0.2978 3/13 [=====>........................] - ETA: 0s - loss: 0.2004  5/13 [==========>...................] - ETA: 0s - loss: 0.1402 6/13 [============>.................] - ETA: 0s - loss: 0.1241 8/13 [=================>............] - ETA: 0s - loss: 0.1187 9/13 [===================>..........] - ETA: 0s - loss: 0.110111/13 [========================>.....] - ETA: 0s - loss: 0.094612/13 [==========================>...] - ETA: 0s - loss: 0.091013/13 [==============================] - 6s 110ms/step - loss: 0.0874 - val_loss: 0.0474 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0560 3/13 [=====>........................] - ETA: 0s - loss: 0.0477 4/13 [========>.....................] - ETA: 0s - loss: 0.0519 6/13 [============>.................] - ETA: 0s - loss: 0.0520 7/13 [===============>..............] - ETA: 0s - loss: 0.0491 8/13 [=================>............] - ETA: 0s - loss: 0.051910/13 [======================>.......] - ETA: 0s - loss: 0.047712/13 [==========================>...] - ETA: 0s - loss: 0.048313/13 [==============================] - ETA: 0s - loss: 0.046913/13 [==============================] - 1s 54ms/step - loss: 0.0469 - val_loss: 0.0877 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0210 3/13 [=====>........................] - ETA: 0s - loss: 0.0360 4/13 [========>.....................] - ETA: 0s - loss: 0.0331 6/13 [============>.................] - ETA: 0s - loss: 0.0423 7/13 [===============>..............] - ETA: 0s - loss: 0.0413 8/13 [=================>............] - ETA: 0s - loss: 0.041910/13 [======================>.......] - ETA: 0s - loss: 0.041912/13 [==========================>...] - ETA: 0s - loss: 0.042613/13 [==============================] - 1s 56ms/step - loss: 0.0427 - val_loss: 0.0497 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0460 3/13 [=====>........................] - ETA: 0s - loss: 0.0337 5/13 [==========>...................] - ETA: 0s - loss: 0.0457 7/13 [===============>..............] - ETA: 0s - loss: 0.0393 9/13 [===================>..........] - ETA: 0s - loss: 0.041811/13 [========================>.....] - ETA: 0s - loss: 0.041913/13 [==============================] - ETA: 0s - loss: 0.042013/13 [==============================] - 1s 56ms/step - loss: 0.0420 - val_loss: 0.0658 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.05963/3 [==============================] - 0s 12ms/step - loss: 0.0663
Test Loss: 0.06632217019796371
Saved model as forecasting_models_v5/model_gru_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for GRU model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 754ms/step
Predicted QoE for the next time step: 53.434150473746065
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/gru_wide.h5
Completed at Πεμ 03 Απρ 2025 08:33:02 πμ EEST
=====================================================

=====================================================
Training gru model with config: bidirectional
Starting at Πεμ 03 Απρ 2025 08:33:02 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type gru                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --bidirectional --attention_units 128
2025-04-03 08:33:02.934328: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:03.144957: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:33:03.885809: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:03.885898: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:03.885910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:33:06.318586: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:07.089587: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:33:07.089613: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:33:07.089757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4983 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building GRU model with self-attention...
Traceback (most recent call last):
  File "timeseries_forecasting_models_v5.py", line 889, in <module>
    main()
  File "timeseries_forecasting_models_v5.py", line 720, in main
    model.summary()
  File "/home/dimitris/.local/lib/python3.8/site-packages/keras/engine/training.py", line 3214, in summary
    raise ValueError(
ValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.
Warning: Original model file not found at /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/model_gru_with_attention.h5
Directory contents:
total 8120
drwxrwxr-x  2 dimitris dimitris    4096 Απρ   3 08:33 .
drwxrwxr-x 11 dimitris dimitris    4096 Απρ   3 08:28 ..
-rw-rw-r--  1 dimitris dimitris  233792 Απρ   3 08:30 dnn_basic.h5
-rw-rw-r--  1 dimitris dimitris  519208 Απρ   3 08:30 dnn_deep.h5
-rw-rw-r--  1 dimitris dimitris  233792 Απρ   3 08:30 dnn_with_elu.h5
-rw-rw-r--  1 dimitris dimitris  233792 Απρ   3 08:30 dnn_with_high_dropout.h5
-rw-rw-r--  1 dimitris dimitris  514616 Απρ   3 08:32 gru_basic.h5
-rw-rw-r--  1 dimitris dimitris  709264 Απρ   3 08:32 gru_deep.h5
-rw-rw-r--  1 dimitris dimitris 1504352 Απρ   3 08:32 gru_wide.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:29 linear_basic.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:30 linear_with_elastic_net.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:29 linear_with_l1_reg.h5
-rw-rw-r--  1 dimitris dimitris   22064 Απρ   3 08:29 linear_with_l2_reg.h5
-rw-rw-r--  1 dimitris dimitris  628760 Απρ   3 08:30 lstm_basic.h5
-rw-rw-r--  1 dimitris dimitris  881952 Απρ   3 08:31 lstm_deep.h5
-rw-rw-r--  1 dimitris dimitris 1914264 Απρ   3 08:31 lstm_wide.h5
-rw-rw-r--  1 dimitris dimitris  628760 Απρ   3 08:31 lstm_with_stats.h5
-rw-rw-r--  1 dimitris dimitris    3463 Απρ   3 08:32 scaler.save
-rw-rw-r--  1 dimitris dimitris  165338 Απρ   3 08:33 training_log.txt
Completed at Πεμ 03 Απρ 2025 08:33:09 πμ EEST
=====================================================

=====================================================
Training gru model with config: with_stats
Starting at Πεμ 03 Απρ 2025 08:33:09 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type gru                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --attention_units 128
2025-04-03 08:33:10.348795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:10.517517: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:33:11.149120: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:11.149229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:11.149248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:33:13.359914: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:14.135250: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:33:14.135276: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:33:14.135442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4985 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building GRU model with self-attention...
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 gru (GRU)                   (None, 5, 50)             14550     
                                                                 
 gru_1 (GRU)                 (None, 5, 50)             15300     
                                                                 
 self_attention (SelfAttenti  (None, 50)               6656      
 on)                                                             
                                                                 
 dense (Dense)               (None, 25)                1275      
                                                                 
 dense_1 (Dense)             (None, 1)                 26        
                                                                 
=================================================================
Total params: 37,807
Trainable params: 37,807
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 57s - loss: 0.2986 3/13 [=====>........................] - ETA: 0s - loss: 0.2421  5/13 [==========>...................] - ETA: 0s - loss: 0.1982 7/13 [===============>..............] - ETA: 0s - loss: 0.1637 9/13 [===================>..........] - ETA: 0s - loss: 0.142711/13 [========================>.....] - ETA: 0s - loss: 0.129213/13 [==============================] - ETA: 0s - loss: 0.120513/13 [==============================] - 6s 108ms/step - loss: 0.1205 - val_loss: 0.1870 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0631 3/13 [=====>........................] - ETA: 0s - loss: 0.0472 5/13 [==========>...................] - ETA: 0s - loss: 0.0422 7/13 [===============>..............] - ETA: 0s - loss: 0.0468 9/13 [===================>..........] - ETA: 0s - loss: 0.043510/13 [======================>.......] - ETA: 0s - loss: 0.044411/13 [========================>.....] - ETA: 0s - loss: 0.044612/13 [==========================>...] - ETA: 0s - loss: 0.045513/13 [==============================] - ETA: 0s - loss: 0.046713/13 [==============================] - 1s 56ms/step - loss: 0.0467 - val_loss: 0.0622 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0554 2/13 [===>..........................] - ETA: 0s - loss: 0.0483 3/13 [=====>........................] - ETA: 0s - loss: 0.0465 4/13 [========>.....................] - ETA: 0s - loss: 0.0500 5/13 [==========>...................] - ETA: 0s - loss: 0.0470 7/13 [===============>..............] - ETA: 0s - loss: 0.0463 9/13 [===================>..........] - ETA: 0s - loss: 0.047210/13 [======================>.......] - ETA: 0s - loss: 0.047512/13 [==========================>...] - ETA: 0s - loss: 0.045713/13 [==============================] - 1s 56ms/step - loss: 0.0462 - val_loss: 0.0819 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0220 3/13 [=====>........................] - ETA: 0s - loss: 0.0323 4/13 [========>.....................] - ETA: 0s - loss: 0.0388 6/13 [============>.................] - ETA: 0s - loss: 0.0436 7/13 [===============>..............] - ETA: 0s - loss: 0.0420 8/13 [=================>............] - ETA: 0s - loss: 0.041410/13 [======================>.......] - ETA: 0s - loss: 0.039712/13 [==========================>...] - ETA: 0s - loss: 0.042013/13 [==============================] - 1s 56ms/step - loss: 0.0421 - val_loss: 0.0578 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0425 3/13 [=====>........................] - ETA: 0s - loss: 0.0394 5/13 [==========>...................] - ETA: 0s - loss: 0.0442 7/13 [===============>..............] - ETA: 0s - loss: 0.0423 8/13 [=================>............] - ETA: 0s - loss: 0.040910/13 [======================>.......] - ETA: 0s - loss: 0.039912/13 [==========================>...] - ETA: 0s - loss: 0.041513/13 [==============================] - 1s 53ms/step - loss: 0.0412 - val_loss: 0.0889 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0272 2/13 [===>..........................] - ETA: 0s - loss: 0.0392 4/13 [========>.....................] - ETA: 0s - loss: 0.0299 6/13 [============>.................] - ETA: 0s - loss: 0.0399 8/13 [=================>............] - ETA: 0s - loss: 0.0416 9/13 [===================>..........] - ETA: 0s - loss: 0.041911/13 [========================>.....] - ETA: 0s - loss: 0.041712/13 [==========================>...] - ETA: 0s - loss: 0.041113/13 [==============================] - 1s 55ms/step - loss: 0.0409 - val_loss: 0.0618 - lr: 0.0010
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0534 3/13 [=====>........................] - ETA: 0s - loss: 0.0481 5/13 [==========>...................] - ETA: 0s - loss: 0.0487 6/13 [============>.................] - ETA: 0s - loss: 0.0455 8/13 [=================>............] - ETA: 0s - loss: 0.045310/13 [======================>.......] - ETA: 0s - loss: 0.046112/13 [==========================>...] - ETA: 0s - loss: 0.044313/13 [==============================] - ETA: 0s - loss: 0.042313/13 [==============================] - 1s 56ms/step - loss: 0.0423 - val_loss: 0.0822 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04043/3 [==============================] - 0s 10ms/step - loss: 0.0437
Test Loss: 0.043680429458618164
Saved model as forecasting_models_v5/model_gru_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for GRU model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 773ms/step
Predicted QoE for the next time step: 89.93685596983075
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/gru_with_stats.h5
Completed at Πεμ 03 Απρ 2025 08:33:28 πμ EEST
=====================================================

=====================================================
Training transformer model with config: basic
Starting at Πεμ 03 Απρ 2025 08:33:28 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type transformer                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 
2025-04-03 08:33:29.409854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:29.604373: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:33:30.338643: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:30.338733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:30.338744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:33:32.685450: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:33.456963: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:33:33.456989: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:33:33.457120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4973 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Transformer model...
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 5, 45)]           0         
                                                                 
 transformer_block (Transfor  (None, 5, 45)            22564     
 merBlock)                                                       
                                                                 
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense_2 (Dense)             (None, 32)                7232      
                                                                 
 dense_3 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 29,829
Trainable params: 29,829
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 20s - loss: 0.2733 8/13 [=================>............] - ETA: 0s - loss: 0.2091 13/13 [==============================] - 2s 37ms/step - loss: 0.1827 - val_loss: 0.0765 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.1323 7/13 [===============>..............] - ETA: 0s - loss: 0.092613/13 [==============================] - 0s 12ms/step - loss: 0.0931 - val_loss: 0.1301 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0843 7/13 [===============>..............] - ETA: 0s - loss: 0.059513/13 [==============================] - 0s 13ms/step - loss: 0.0654 - val_loss: 0.0884 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0744 7/13 [===============>..............] - ETA: 0s - loss: 0.070213/13 [==============================] - ETA: 0s - loss: 0.062113/13 [==============================] - 0s 13ms/step - loss: 0.0621 - val_loss: 0.0598 - lr: 5.0000e-04
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0560 8/13 [=================>............] - ETA: 0s - loss: 0.059913/13 [==============================] - 0s 12ms/step - loss: 0.0530 - val_loss: 0.0668 - lr: 5.0000e-04
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0445 8/13 [=================>............] - ETA: 0s - loss: 0.052913/13 [==============================] - 0s 12ms/step - loss: 0.0547 - val_loss: 0.0989 - lr: 5.0000e-04
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0671 8/13 [=================>............] - ETA: 0s - loss: 0.055113/13 [==============================] - 0s 14ms/step - loss: 0.0538 - val_loss: 0.0819 - lr: 2.5000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.05013/3 [==============================] - 0s 4ms/step - loss: 0.0524
Test Loss: 0.0524090975522995
Saved model as forecasting_models_v5/model_transformer_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for TRANSFORMER model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 193ms/step
Predicted QoE for the next time step: 192.01912130551574
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/transformer_basic.h5
Completed at Πεμ 03 Απρ 2025 08:33:40 πμ EEST
=====================================================

=====================================================
Training transformer model with config: more_heads
Starting at Πεμ 03 Απρ 2025 08:33:40 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type transformer                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --num_heads 4
2025-04-03 08:33:40.656003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:40.849420: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:33:41.532737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:41.532830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:41.532841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:33:43.737485: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:44.479953: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:33:44.479980: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:33:44.480122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4949 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Transformer model...
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 5, 45)]           0         
                                                                 
 transformer_block (Transfor  (None, 5, 45)            39034     
 merBlock)                                                       
                                                                 
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense_2 (Dense)             (None, 32)                7232      
                                                                 
 dense_3 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 46,299
Trainable params: 46,299
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 18s - loss: 0.8785 8/13 [=================>............] - ETA: 0s - loss: 0.3042 13/13 [==============================] - 2s 30ms/step - loss: 0.2495 - val_loss: 0.1045 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.1424 8/13 [=================>............] - ETA: 0s - loss: 0.121413/13 [==============================] - 0s 11ms/step - loss: 0.1068 - val_loss: 0.1529 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0653 8/13 [=================>............] - ETA: 0s - loss: 0.062013/13 [==============================] - 0s 12ms/step - loss: 0.0646 - val_loss: 0.0603 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0457 7/13 [===============>..............] - ETA: 0s - loss: 0.057113/13 [==============================] - 0s 11ms/step - loss: 0.0536 - val_loss: 0.0709 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0344 8/13 [=================>............] - ETA: 0s - loss: 0.053313/13 [==============================] - 0s 12ms/step - loss: 0.0512 - val_loss: 0.0556 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0275 7/13 [===============>..............] - ETA: 0s - loss: 0.047313/13 [==============================] - ETA: 0s - loss: 0.050413/13 [==============================] - 0s 13ms/step - loss: 0.0504 - val_loss: 0.0718 - lr: 0.0010
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0196 7/13 [===============>..............] - ETA: 0s - loss: 0.043913/13 [==============================] - 0s 12ms/step - loss: 0.0453 - val_loss: 0.0533 - lr: 0.0010
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0356 8/13 [=================>............] - ETA: 0s - loss: 0.043413/13 [==============================] - 0s 12ms/step - loss: 0.0472 - val_loss: 0.0685 - lr: 0.0010
Epoch 9/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0198 7/13 [===============>..............] - ETA: 0s - loss: 0.049113/13 [==============================] - 0s 12ms/step - loss: 0.0455 - val_loss: 0.0689 - lr: 0.0010
Epoch 10/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0498 8/13 [=================>............] - ETA: 0s - loss: 0.047613/13 [==============================] - 0s 15ms/step - loss: 0.0420 - val_loss: 0.0589 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.03733/3 [==============================] - 0s 3ms/step - loss: 0.0411
Test Loss: 0.041125062853097916
Saved model as forecasting_models_v5/model_transformer_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for TRANSFORMER model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 185ms/step
Predicted QoE for the next time step: 66.81729520381927
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/transformer_more_heads.h5
Completed at Πεμ 03 Απρ 2025 08:33:51 πμ EEST
=====================================================

=====================================================
Training transformer model with config: large_ff
Starting at Πεμ 03 Απρ 2025 08:33:51 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type transformer                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --ff_dim 128
2025-04-03 08:33:51.943857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:52.098598: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:33:52.724362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:52.724448: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:33:52.724459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:33:54.995165: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:33:55.800626: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:33:55.800670: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:33:55.800913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4925 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Transformer model...
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 5, 45)]           0         
                                                                 
 transformer_block (Transfor  (None, 5, 45)            28388     
 merBlock)                                                       
                                                                 
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense_2 (Dense)             (None, 32)                7232      
                                                                 
 dense_3 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 35,653
Trainable params: 35,653
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 18s - loss: 0.8647 8/13 [=================>............] - ETA: 0s - loss: 0.3066 13/13 [==============================] - 2s 32ms/step - loss: 0.2703 - val_loss: 0.1142 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0896 8/13 [=================>............] - ETA: 0s - loss: 0.104213/13 [==============================] - 0s 11ms/step - loss: 0.1034 - val_loss: 0.0992 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0575 8/13 [=================>............] - ETA: 0s - loss: 0.078613/13 [==============================] - 0s 13ms/step - loss: 0.0775 - val_loss: 0.0964 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0281 7/13 [===============>..............] - ETA: 0s - loss: 0.056713/13 [==============================] - ETA: 0s - loss: 0.063113/13 [==============================] - 0s 13ms/step - loss: 0.0631 - val_loss: 0.1548 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0803 7/13 [===============>..............] - ETA: 0s - loss: 0.070413/13 [==============================] - 0s 13ms/step - loss: 0.0689 - val_loss: 0.0864 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0329 7/13 [===============>..............] - ETA: 0s - loss: 0.052313/13 [==============================] - ETA: 0s - loss: 0.053413/13 [==============================] - 0s 14ms/step - loss: 0.0534 - val_loss: 0.0638 - lr: 0.0010
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.1139 7/13 [===============>..............] - ETA: 0s - loss: 0.054513/13 [==============================] - ETA: 0s - loss: 0.053413/13 [==============================] - 0s 14ms/step - loss: 0.0534 - val_loss: 0.0766 - lr: 0.0010
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0725 7/13 [===============>..............] - ETA: 0s - loss: 0.046213/13 [==============================] - 0s 13ms/step - loss: 0.0452 - val_loss: 0.0766 - lr: 0.0010
Epoch 9/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0410 8/13 [=================>............] - ETA: 0s - loss: 0.040013/13 [==============================] - 0s 14ms/step - loss: 0.0419 - val_loss: 0.0769 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.05233/3 [==============================] - 0s 4ms/step - loss: 0.0585
Test Loss: 0.05850791186094284
Saved model as forecasting_models_v5/model_transformer_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for TRANSFORMER model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 225ms/step
Predicted QoE for the next time step: 12.209175171306132
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/transformer_large_ff.h5
Completed at Πεμ 03 Απρ 2025 08:34:02 πμ EEST
=====================================================

=====================================================
Training transformer model with config: low_dropout
Starting at Πεμ 03 Απρ 2025 08:34:02 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type transformer                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 --dropout_rate 0.05
2025-04-03 08:34:03.258507: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:34:03.440818: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:34:04.150597: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:34:04.150699: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:34:04.150715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:34:06.339216: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:34:07.065551: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:34:07.065577: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:34:07.065739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4954 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Transformer model...
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 5, 45)]           0         
                                                                 
 transformer_block (Transfor  (None, 5, 45)            22564     
 merBlock)                                                       
                                                                 
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense_2 (Dense)             (None, 32)                7232      
                                                                 
 dense_3 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 29,829
Trainable params: 29,829
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 19s - loss: 0.3762 8/13 [=================>............] - ETA: 0s - loss: 0.2583 13/13 [==============================] - 2s 37ms/step - loss: 0.2389 - val_loss: 0.1121 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0616 8/13 [=================>............] - ETA: 0s - loss: 0.096413/13 [==============================] - 0s 11ms/step - loss: 0.0929 - val_loss: 0.0942 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.1202 8/13 [=================>............] - ETA: 0s - loss: 0.078513/13 [==============================] - 0s 11ms/step - loss: 0.0705 - val_loss: 0.0628 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0495 8/13 [=================>............] - ETA: 0s - loss: 0.054513/13 [==============================] - 0s 11ms/step - loss: 0.0530 - val_loss: 0.0750 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0673 8/13 [=================>............] - ETA: 0s - loss: 0.043513/13 [==============================] - 0s 12ms/step - loss: 0.0436 - val_loss: 0.0629 - lr: 0.0010
Epoch 6/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0258 7/13 [===============>..............] - ETA: 0s - loss: 0.043813/13 [==============================] - 0s 12ms/step - loss: 0.0438 - val_loss: 0.0598 - lr: 5.0000e-04
Epoch 7/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0379 8/13 [=================>............] - ETA: 0s - loss: 0.035813/13 [==============================] - 0s 13ms/step - loss: 0.0366 - val_loss: 0.0574 - lr: 5.0000e-04
Epoch 8/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0213 7/13 [===============>..............] - ETA: 0s - loss: 0.035113/13 [==============================] - ETA: 0s - loss: 0.037813/13 [==============================] - 0s 13ms/step - loss: 0.0378 - val_loss: 0.0886 - lr: 5.0000e-04
Epoch 9/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0318 7/13 [===============>..............] - ETA: 0s - loss: 0.040313/13 [==============================] - ETA: 0s - loss: 0.039013/13 [==============================] - 0s 13ms/step - loss: 0.0390 - val_loss: 0.0625 - lr: 5.0000e-04
Epoch 10/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0225 8/13 [=================>............] - ETA: 0s - loss: 0.036213/13 [==============================] - 0s 12ms/step - loss: 0.0342 - val_loss: 0.0555 - lr: 2.5000e-04
Epoch 11/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0188 7/13 [===============>..............] - ETA: 0s - loss: 0.026313/13 [==============================] - ETA: 0s - loss: 0.029713/13 [==============================] - 0s 13ms/step - loss: 0.0297 - val_loss: 0.0550 - lr: 2.5000e-04
Epoch 12/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0207 7/13 [===============>..............] - ETA: 0s - loss: 0.026913/13 [==============================] - ETA: 0s - loss: 0.029513/13 [==============================] - 0s 13ms/step - loss: 0.0295 - val_loss: 0.0600 - lr: 2.5000e-04
Epoch 13/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0363 7/13 [===============>..............] - ETA: 0s - loss: 0.029213/13 [==============================] - 0s 12ms/step - loss: 0.0321 - val_loss: 0.0660 - lr: 2.5000e-04
Epoch 14/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0164 7/13 [===============>..............] - ETA: 0s - loss: 0.022813/13 [==============================] - ETA: 0s - loss: 0.030613/13 [==============================] - 0s 15ms/step - loss: 0.0306 - val_loss: 0.0651 - lr: 1.2500e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04323/3 [==============================] - 0s 4ms/step - loss: 0.0485
Test Loss: 0.048513416200876236
Saved model as forecasting_models_v5/model_transformer_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for TRANSFORMER model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 189ms/step
Predicted QoE for the next time step: 39.57760339433193
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/transformer_low_dropout.h5
Completed at Πεμ 03 Απρ 2025 08:34:14 πμ EEST
=====================================================

=====================================================
Training transformer model with config: with_stats
Starting at Πεμ 03 Απρ 2025 08:34:14 πμ EEST
=====================================================
Command: python3 timeseries_forecasting_models_v5.py                 --data_folder ./real_dataset                 --model_type transformer                 --seq_length 5                 --epochs 20                 --batch_size 16                 --augmented                 --use_stats                 
2025-04-03 08:34:15.213433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:34:15.379131: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-03 08:34:16.029153: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:34:16.029243: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/local/cuda-11.2/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:
2025-04-03 08:34:16.029254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2025-04-03 08:34:18.224319: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-03 08:34:18.946119: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2025-04-03 08:34:18.946144: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2025-04-03 08:34:18.946281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4957 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5
Loading augmented dataset from: ./real_dataset
Using new format
Dataset shape: (329, 47)
Features: 45
Sample timestamp: 2025-04-02 12:19:54
Total sequences: 324
Input shape: (324, 5, 45)
Training samples: 259 Test samples: 65
Building Transformer model...
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 5, 45)]           0         
                                                                 
 transformer_block (Transfor  (None, 5, 45)            22564     
 merBlock)                                                       
                                                                 
 flatten (Flatten)           (None, 225)               0         
                                                                 
 dense_2 (Dense)             (None, 32)                7232      
                                                                 
 dense_3 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 29,829
Trainable params: 29,829
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
 1/13 [=>............................] - ETA: 19s - loss: 1.0430 7/13 [===============>..............] - ETA: 0s - loss: 0.3865 13/13 [==============================] - ETA: 0s - loss: 0.282813/13 [==============================] - 2s 35ms/step - loss: 0.2828 - val_loss: 0.1191 - lr: 0.0010
Epoch 2/20
 1/13 [=>............................] - ETA: 0s - loss: 0.1234 8/13 [=================>............] - ETA: 0s - loss: 0.095613/13 [==============================] - 0s 12ms/step - loss: 0.0971 - val_loss: 0.0746 - lr: 0.0010
Epoch 3/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0527 7/13 [===============>..............] - ETA: 0s - loss: 0.075213/13 [==============================] - ETA: 0s - loss: 0.077413/13 [==============================] - 0s 13ms/step - loss: 0.0774 - val_loss: 0.1059 - lr: 0.0010
Epoch 4/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0805 8/13 [=================>............] - ETA: 0s - loss: 0.067013/13 [==============================] - ETA: 0s - loss: 0.068013/13 [==============================] - 0s 13ms/step - loss: 0.0680 - val_loss: 0.1647 - lr: 0.0010
Epoch 5/20
 1/13 [=>............................] - ETA: 0s - loss: 0.0736 8/13 [=================>............] - ETA: 0s - loss: 0.062013/13 [==============================] - 0s 15ms/step - loss: 0.0627 - val_loss: 0.1207 - lr: 5.0000e-04
1/3 [=========>....................] - ETA: 0s - loss: 0.04463/3 [==============================] - 0s 4ms/step - loss: 0.0557
Test Loss: 0.055728767067193985
Saved model as forecasting_models_v5/model_transformer_with_attention.h5
Saved scaler as forecasting_models_v5/scaler.save

Preparing sample inference example for TRANSFORMER model...
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 191ms/step
Predicted QoE for the next time step: 22.24502440544903
Model renamed to: /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/transformer_with_stats.h5
Completed at Πεμ 03 Απρ 2025 08:34:25 πμ EEST
=====================================================
Scaler file exists at /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5/scaler.save

=====================================================
All models trained successfully!
Models are saved in /home/dimitris/Impact-xG_prediction_model/forecasting_models_v5
Training completed at Πεμ 03 Απρ 2025 08:34:25 πμ EEST
=====================================================
